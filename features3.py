import os
import asyncio
from typing import Dict, List, Optional, TypedDict, Annotated
from dataclasses import dataclass, asdict
import logging
from datetime import datetime

# LangGraph imports
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# LangChain imports
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Monitoring
from langfuse import Langfuse
#from langfuse.langchain import CallbackHandler

# Streamlit (si vous gardez l'interface)
import streamlit as st
from dotenv import load_dotenv

# Vos prompts existants
from prompts import (
    BASE_CONNAISSANCES_BLOOM,
    BASE_CONNAISSANCES_PEDAGOGIQUES,
    PROMPT_CLASSIFICATION_BLOOM,
    PROMPT_EVALUATION_OBJECTIFS,
    PROMPT_AUTO_EVAL_EVALUATION,
    PROMPT_AMELIORER_OBJECTIFS,
    PROMPT_AUTO_EVAL_SUGGESTIONS,
    PROMPT_SYNTHESE,
    PROMPT_RECAPITULATIF
)

load_dotenv()

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration Langfuse
langfuse = Langfuse(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST")
)
"""
langfuse_handler = CallbackHandler(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST")
)"""

# Configuration des mod√®les
@dataclass
class ModelConfig:
    api_key: str
    model_name: str = "gemini-2.5-flash"
    temperature: float = 0.4
    max_output_tokens: int = 2024

# √âtat du graphe
class AgentState(TypedDict):
    # Donn√©es d'entr√©e
    nom_cours: str
    niveau: str
    public: str
    objectif_general: str
    objectifs_specifiques: List[str]
    
    # R√©sultats interm√©diaires
    bloom_classification: Optional[str]
    evaluation_objectifs: Optional[str]
    evaluation_revisee: Optional[str]
    suggestions: Optional[str]
    suggestions_revisees: Optional[str]
    synthese_finale: Optional[str]
    
    # M√©tadonn√©es
    messages: Annotated[List, add_messages]
    errors: List[str]
    current_step: str
    retry_count: int
    max_retries: int
    
    # R√©sultat final
    rapport_final: Optional[Dict]

class PedagogicalAgent:
    def __init__(self):
        self.models = {
            "classification": self._create_model(os.getenv("GEMINI_API_KEY_CLASSIFICATION")),
            "evaluation": self._create_model(os.getenv("GEMINI_API_KEY_EVALUATION")),
            "suggestion": self._create_model(os.getenv("GEMINI_API_KEY_SUGGESTION")),
            "synthese": self._create_model(os.getenv("GEMINI_API_KEY_RECAP_SYNTHESE"))
        }
        
        # Cr√©ation du graphe
        self.workflow = self._create_workflow()
        self.app = self.workflow.compile(
            checkpointer=MemorySaver(),
            interrupt_before=[],  # Pas d'interruption par d√©faut
            debug=True
        )
    
    def _create_model(self, api_key: str) -> ChatGoogleGenerativeAI:
        """Cr√©e un mod√®le configur√© avec gestion d'erreurs"""
        return ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            google_api_key=api_key,
            temperature=0.4,
            max_output_tokens=2024,
            #callbacks=[langfuse_handler]
        )
    
    def _create_workflow(self) -> StateGraph:
        """Cr√©e le workflow LangGraph"""
        workflow = StateGraph(AgentState)
        
        # Ajout des n≈ìuds (sans auto_eval_suggestions selon le code original)
        workflow.add_node("classify_bloom", self._classify_bloom_node)
        workflow.add_node("evaluate_objectives", self._evaluate_objectives_node)
        workflow.add_node("auto_eval_evaluation", self._auto_eval_evaluation_node)
        workflow.add_node("generate_suggestions", self._generate_suggestions_node)
        workflow.add_node("create_synthesis", self._create_synthesis_node)
        workflow.add_node("handle_error", self._handle_error_node)
        workflow.add_node("finalize_report", self._finalize_report_node)
        
        # D√©finition des ar√™tes
        workflow.add_edge(START, "classify_bloom")
        workflow.add_conditional_edges(
            "classify_bloom",
            self._should_continue_or_retry,
            {
                "continue": "evaluate_objectives",
                "retry": "classify_bloom",
                "error": "handle_error"
            }
        )
        
        workflow.add_conditional_edges(
            "evaluate_objectives",
            self._should_continue_or_retry,
            {
                "continue": "auto_eval_evaluation",
                "retry": "evaluate_objectives",
                "error": "handle_error"
            }
        )
        
        workflow.add_conditional_edges(
            "auto_eval_evaluation",
            self._should_continue_or_retry,
            {
                "continue": "generate_suggestions",
                "retry": "auto_eval_evaluation",
                "error": "handle_error"
            }
        )
        
        workflow.add_conditional_edges(
            "generate_suggestions",
            self._should_continue_or_retry,
            {
                "continue": "create_synthesis",  # Passer directement √† la synth√®se
                "retry": "generate_suggestions",
                "error": "handle_error"
            }
        )
        
        workflow.add_conditional_edges(
            "create_synthesis",
            self._should_continue_or_retry,
            {
                "continue": "finalize_report",
                "retry": "create_synthesis",
                "error": "handle_error"
            }
        )
        
        workflow.add_edge("finalize_report", END)
        workflow.add_edge("handle_error", END)
        
        return workflow
    
    def _should_continue_or_retry(self, state: AgentState) -> str:
        """D√©cide si on continue, retry ou g√®re l'erreur"""
        current_step = state["current_step"]
        
        # V√©rifier si l'√©tape actuelle a produit un r√©sultat
        step_results = {
            "classify_bloom": state.get("bloom_classification"),
            "evaluate_objectives": state.get("evaluation_objectifs"),
            "auto_eval_evaluation": state.get("evaluation_revisee"),
            "generate_suggestions": state.get("suggestions"),
            "create_synthesis": state.get("synthese_finale")
        }
        
        current_result = step_results.get(current_step)
        
        if current_result is not None and current_result.strip():
            return "continue"
        elif state["retry_count"] < state["max_retries"]:
            return "retry"
        else:
            return "error"
    
    async def _classify_bloom_node(self, state: AgentState) -> AgentState:
        """N≈ìud de classification Bloom"""
        state["current_step"] = "classify_bloom"
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", "Tu es un expert en taxonomie de Bloom r√©vis√©e."),
                ("human", PROMPT_CLASSIFICATION_BLOOM)
            ])
            
            chain = prompt | self.models["classification"] | StrOutputParser()
            
            result = await chain.ainvoke({
                "base_connaissances": BASE_CONNAISSANCES_BLOOM,
                "objectif_general": state["objectif_general"],
                "objectifs_specifiques": "\n".join(f"- {obj}" for obj in state["objectifs_specifiques"])
            })
            
            state["bloom_classification"] = result
            state["messages"].append(AIMessage(content=f"Classification Bloom termin√©e: {len(result)} caract√®res"))
            logger.info("Classification Bloom r√©ussie")
            
        except Exception as e:
            state["errors"].append(f"Erreur classification Bloom: {str(e)}")
            state["retry_count"] += 1
            logger.error(f"Erreur classification Bloom: {e}")
            
        return state
    
    async def _evaluate_objectives_node(self, state: AgentState) -> AgentState:
        """N≈ìud d'√©valuation des objectifs"""
        state["current_step"] = "evaluate_objectives"
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", "Tu es un expert en p√©dagogie universitaire."),
                ("human", PROMPT_EVALUATION_OBJECTIFS)
            ])
            
            chain = prompt | self.models["evaluation"] | StrOutputParser()
            
            result = await chain.ainvoke({
                "base_connaissances": BASE_CONNAISSANCES_PEDAGOGIQUES,
                "nom_cours": state["nom_cours"],
                "niveau": state["niveau"],
                "public": state["public"],
                "bloom_classification": state["bloom_classification"]
            })
            
            state["evaluation_objectifs"] = result
            state["messages"].append(AIMessage(content="√âvaluation des objectifs termin√©e"))
            logger.info("√âvaluation des objectifs r√©ussie")
            
        except Exception as e:
            state["errors"].append(f"Erreur √©valuation objectifs: {str(e)}")
            state["retry_count"] += 1
            logger.error(f"Erreur √©valuation objectifs: {e}")
            
        return state
    
    async def _auto_eval_evaluation_node(self, state: AgentState) -> AgentState:
        """N≈ìud d'auto-√©valuation de l'√©valuation"""
        state["current_step"] = "auto_eval_evaluation"
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", "Tu es un expert en p√©dagogie universitaire."),
                ("human", PROMPT_AUTO_EVAL_EVALUATION)
            ])
            
            chain = prompt | self.models["evaluation"] | StrOutputParser()
            
            result = await chain.ainvoke({
                "base_connaissances": BASE_CONNAISSANCES_PEDAGOGIQUES,
                "evaluation": state["evaluation_objectifs"]
            })
            
            state["evaluation_revisee"] = result
            state["messages"].append(AIMessage(content="Auto-√©valuation termin√©e"))
            logger.info("Auto-√©valuation r√©ussie")
            
        except Exception as e:
            state["errors"].append(f"Erreur auto-√©valuation: {str(e)}")
            state["retry_count"] += 1
            logger.error(f"Erreur auto-√©valuation: {e}")
            
        return state
    
    async def _generate_suggestions_node(self, state: AgentState) -> AgentState:
        """N≈ìud de g√©n√©ration de suggestions"""
        state["current_step"] = "generate_suggestions"
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", "Tu es un expert en p√©dagogie universitaire."),
                ("human", PROMPT_AMELIORER_OBJECTIFS)
            ])
            
            chain = prompt | self.models["suggestion"] | StrOutputParser()
            
            result = await chain.ainvoke({
                "base_connaissances": BASE_CONNAISSANCES_PEDAGOGIQUES,
                "nom_cours": state["nom_cours"],
                "niveau": state["niveau"],
                "public": state["public"],
                "evaluation_objectifs": state["evaluation_revisee"]
            })
            
            state["suggestions"] = result
            state["messages"].append(AIMessage(content="Suggestions g√©n√©r√©es"))
            logger.info("G√©n√©ration de suggestions r√©ussie")
            
        except Exception as e:
            state["errors"].append(f"Erreur g√©n√©ration suggestions: {str(e)}")
            state["retry_count"] += 1
            logger.error(f"Erreur g√©n√©ration suggestions: {e}")
            
        return state
    
    async def _auto_eval_suggestions_node(self, state: AgentState) -> AgentState:
        """N≈ìud d'auto-√©valuation des suggestions"""
        state["current_step"] = "auto_eval_suggestions"
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", "Tu es un expert en p√©dagogie universitaire."),
                ("human", PROMPT_AUTO_EVAL_SUGGESTIONS)
            ])
            
            chain = prompt | self.models["suggestion"] | StrOutputParser()
            
            result = await chain.ainvoke({
                "base_connaissances": BASE_CONNAISSANCES_PEDAGOGIQUES,
                "suggestions": state["suggestions"]
            })
            
            state["suggestions_revisees"] = result
            state["messages"].append(AIMessage(content="Auto-√©valuation des suggestions termin√©e"))
            logger.info("Auto-√©valuation des suggestions r√©ussie")
            
        except Exception as e:
            state["errors"].append(f"Erreur auto-√©valuation suggestions: {str(e)}")
            state["retry_count"] += 1
            logger.error(f"Erreur auto-√©valuation suggestions: {e}")
            
        return state
    
    async def _create_synthesis_node(self, state: AgentState) -> AgentState:
        """N≈ìud de cr√©ation de synth√®se"""
        state["current_step"] = "create_synthesis"
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", "Tu es un expert en p√©dagogie universitaire."),
                ("human", PROMPT_SYNTHESE)
            ])
            
            chain = prompt | self.models["synthese"] | StrOutputParser()
            
            # Utiliser les suggestions finales comme rapport (comme dans le code original)
            rapport = state["suggestions"]
            
            result = await chain.ainvoke({
                "nom_cours": state["nom_cours"],
                "niveau": state["niveau"],
                "public": state["public"],
                "rapport": rapport
            })
            
            state["synthese_finale"] = result
            state["messages"].append(AIMessage(content="Synth√®se finale cr√©√©e"))
            logger.info("Cr√©ation de synth√®se r√©ussie")
            
        except Exception as e:
            state["errors"].append(f"Erreur cr√©ation synth√®se: {str(e)}")
            state["retry_count"] += 1
            logger.error(f"Erreur cr√©ation synth√®se: {e}")
            
        return state
    
    async def _finalize_report_node(self, state: AgentState) -> AgentState:
        """N≈ìud de finalisation du rapport - Format identique au code original"""
        try:
            # Format exact du code original
            rapport = f"""
            {state["suggestions"]}
            """
            
            resultat_final = {
                "informations_cours": f"""
                **Cours :** {state["nom_cours"]}
                **Niveau :** {state["niveau"]}
                **Public cible :** {state["public"]} 
                **Objectif g√©n√©ral :** 
                    {state["objectif_general"]}
                **Objectifs specifiques :** 
                    {state["objectifs_specifiques"]}
                """,
                "aper√ßu": state["synthese_finale"],
                "details": rapport
            }
            
            state["rapport_final"] = resultat_final
            state["messages"].append(AIMessage(content="Rapport final cr√©√© avec succ√®s"))
            logger.info("Rapport final cr√©√© avec succ√®s")
            
        except Exception as e:
            state["errors"].append(f"Erreur finalisation rapport: {str(e)}")
            logger.error(f"Erreur finalisation rapport: {e}")
            
        return state
    
    async def _handle_error_node(self, state: AgentState) -> AgentState:
        """N≈ìud de gestion des erreurs"""
        error_msg = f"Erreur dans l'√©tape {state['current_step']}: {'; '.join(state['errors'])}"
        
        state["rapport_final"] = {
            "error": True,
            "message": "‚ùå Une erreur est survenue lors de l'analyse.",
            "details": error_msg,
            "current_step": state["current_step"],
            "retry_count": state["retry_count"],
            "timestamp": datetime.now().isoformat()
        }
        
        state["messages"].append(AIMessage(content=error_msg))
        logger.error(error_msg)
        
        return state
    
    async def run_analysis(self, **kwargs) -> Dict:
        """Lance l'analyse p√©dagogique"""
        initial_state = {
            "nom_cours": kwargs.get("nom_cours", ""),
            "niveau": kwargs.get("niveau", ""),
            "public": kwargs.get("public", ""),
            "objectif_general": kwargs.get("objectif_general", ""),
            "objectifs_specifiques": kwargs.get("objectifs_specifiques", []),
            "bloom_classification": None,
            "evaluation_objectifs": None,
            "evaluation_revisee": None,
            "suggestions": None,
            "suggestions_revisees": None,
            "synthese_finale": None,
            "messages": [HumanMessage(content="D√©but de l'analyse p√©dagogique")],
            "errors": [],
            "current_step": "",
            "retry_count": 0,
            "max_retries": 3,
            "rapport_final": None
        }
        
        config = {"configurable": {"thread_id": f"analysis_{datetime.now().timestamp()}"}}
        
        try:
            # Ex√©cution asynchrone du workflow
            async for event in self.app.astream(initial_state, config=config):
                if hasattr(st, 'info'):  # Si Streamlit est disponible
                    for node_name, node_state in event.items():
                        if node_name != "__end__":
                            step_name = {
                                "classify_bloom": "Classification selon Bloom",
                                "evaluate_objectives": "√âvaluation des objectifs",
                                "auto_eval_evaluation": "R√©vision de l'√©valuation",
                                "generate_suggestions": "G√©n√©ration de recommandations",
                                "create_synthesis": "Cr√©ation de la synth√®se",
                                "finalize_report": "Finalisation du rapport"
                            }.get(node_name, node_name)
                            
                            st.info(f"üîÑ {step_name}...")
            
            # R√©cup√©ration du r√©sultat final
            final_state = await self.app.aget_state(config)
            return final_state.values.get("rapport_final", {
                "error": True,
                "message": "Aucun r√©sultat produit"
            })
            
        except Exception as e:
            logger.error(f"Erreur lors de l'ex√©cution du workflow: {e}")
            return {
                "error": True,
                "message": f"‚ùå Erreur lors de l'ex√©cution: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }

# Fonction d'interface pour Streamlit
def assistant_pedagogique(nom_cours, niveau, public, objectif_general, objectifs_specifiques):
    """Interface pour Streamlit utilisant LangGraph"""
    agent = PedagogicalAgent()
    
    # Ex√©cution synchrone pour Streamlit
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        result = loop.run_until_complete(
            agent.run_analysis(
                nom_cours=nom_cours,
                niveau=niveau,
                public=public,
                objectif_general=objectif_general,
                objectifs_specifiques=objectifs_specifiques
            )
        )
        return result
    finally:
        loop.close()

# Fonction pour r√©capitulatif (identique au code original)
async def recapitulatif(rapport: str, api_key: str) -> str:
    """Fonction r√©capitulatif identique au code original"""
    prompt = f"""
    Tu es un assistant p√©dagogique expert. √Ä partir du rapport suivant, g√©n√®re une synth√®se structur√©e dans un dictionnaire Python avec les √©l√©ments suivants :
        
    - **points_forts** : une liste des √©l√©ments positifs remarqu√©s dans la formulation des objectifs (clart√©, niveau de Bloom, ad√©quation au contenu du cours, etc.)

    - **axes_amelioration** : une liste synth√©tique des am√©liorations g√©n√©rales sugg√©r√©es (par exemple : "Certains objectifs ne sont pas assez pr√©cis, ce qui rend leur compr√©hension et leur √©valuation difficiles.", "Certains verbes utilis√©s ne refl√®tent pas le niveau attendu selon la taxonomie de Bloom.", etc.)

    - **objectifs_total** : le nombre total d'objectifs analys√©s.

    - **objectifs_conformes** :
        - `nbre_total` : nombre d'objectifs conformes,
        - `liste` : une liste de dictionnaires, chacun contenant :
            - `num` : le num√©ro de l'objectif,
            - `objectif` : le texte de l'objectif,
            - `niveau_bloom` : le niveau de la taxonomie de Bloom identifi√©.

    - **objectifs_a_ameliorer** :
        - `nbre_total` : nombre d'objectifs √† corriger,
        - `liste` : une liste de dictionnaires, chacun contenant :
            - `num` : le num√©ro de l'objectif,
            - `objectif` : le texte original,
            - `probleme_resume` : r√©sum√© du d√©faut,
            - `suggestion` : r√©sum√© de la proposition de reformulation ou d'am√©lioration.

    - **recommandations** : un r√©sum√©, sous forme de liste, des recommandations g√©n√©rales √† l'intention de l'enseignant pour am√©liorer la formulation des objectifs.

    - **niveaux_bloom_utilises** : une liste r√©capitulative des niveaux de la taxonomie de Bloom identifi√©s dans l'ensemble des objectifs, qu'ils soient objectif g√©n√©ral comme sp√©cifique, conforme comme √† am√©liorer. Ex : ['Conna√Ætre', 'Comprendre', etc]
    
    Les objectifs conformes sont ceux qui ont 5/5 pour tous les crit√®res.
    Les objectifs √† am√©liorer sont ceux qui n'ont pas obtenu 5/5 pour tous les crit√®res.
    
    Voici le rapport √† analyser :

    {rapport}

    R√©ponds uniquement avec un objet Python de type `dict` valide. Aucune explication. Pas de texte hors du dictionnaire.
    """
    
    model = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        google_api_key=api_key,
        temperature=0.4,
        max_output_tokens=2024,
        #callbacks=[langfuse_handler]
    )
    
    return await model.ainvoke(prompt)

# Exemple d'utilisation
if __name__ == "__main__":
    # Test avec donn√©es d'exemple
    test_data = {
        "nom_cours": "Introduction √† la Programmation Python",
        "niveau": "Licence 1",
        "public": "√âtudiants d√©butants en informatique",
        "objectif_general": "Ma√Ætriser les bases de la programmation Python",
        "objectifs_specifiques": [
            "Comprendre la syntaxe de base de Python",
            "Savoir utiliser les structures de contr√¥le",
            "√ätre capable de cr√©er des fonctions simples"
        ]
    }
    
    # Ex√©cution asynchrone
    async def main():
        result = await run_pedagogical_analysis(**test_data)
        print("R√©sultat de l'analyse:")
        print(result)
    
    asyncio.run(main())